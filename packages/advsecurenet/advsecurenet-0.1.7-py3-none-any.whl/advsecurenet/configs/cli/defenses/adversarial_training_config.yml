# Description: The configuration file for adversarial training.

# --- Target Model and Attack Configurations ---
model: resnet18 # The model that will be trained.
models: null # List of models used for generating adversarial examples.
attacks: # List of attacks used for generating adversarial examples.
  - Attack1: # Name of the attack.
      config: path/to/attack1_config.yml # Path to the attack configuration file.

# --- Dataset Configurations ---
dataset_type: cifar10 # The dataset on which the model will be trained.
num_classes: 10 # Number of classes in the dataset.
train_dataset_path: null # Path to the train dataset. Required if dataset_type is custom.
test_dataset_path: null # Path to the test dataset. Required if dataset_type is custom.
num_samples_train: null # Number of samples you want to use from the train dataset. If null, uses all samples.
num_samples_test: null # Number of samples you want to use from the test dataset. If null, uses all samples.

# --- Training Configurations ---
epochs: 10 # Number of epochs to train for.
batch_size: 32 # Batch size for training. If using ddp, this will be divided equally among all GPUs. Consider having batch_size_per_gpu * num_gpus in case of ddp.
lr: 0.001 # Learning rate for training.
optimizer: adam #
optimizer_kwargs: # Optional arguments for the optimizer. If not provided, default values will be used.
  weight_decay: 0.0001
scheduler: LINEAR_LR # Scheduler to use for training. Options: [CONSTANT_LR, LINEAR_LR, STEP_LR, EXP_LR, COSINE_LR] If not provided, no scheduler will be used.
scheduler_kwargs: null # Optional arguments for the scheduler. If not provided, default values will be used.
loss: cross_entropy

# --- DataLoader Configurations ---
num_workers_train: 4 # Number of workers for the train dataloader.
num_workers_test: 4 # Number of workers for the test dataloader.
shuffle_train: True # Whether to shuffle the train dataloader. If using ddp this should be false.
shuffle_test: false # Whether to shuffle the test dataloader.
drop_last_train: False # Whether to drop the last batch for the train dataloader.
drop_last_test: False # Whether to drop the last batch for the test dataloader.

# --- Device Configuration ---
use_ddp: False # Whether to use multi-GPU training with DistributedDataParallel.

# --- Device Configuration MULTI GPU ONLY ---
# This configuration is only for multi-GPU training. For single GPU training, use the previous configuration. Make sure to set use_ddp to True.
gpu_ids: null # List of GPU IDs to use for training. If null, uses all available GPUs.
pin_memory: True # Whether to pin memory for the dataloader.

# --- Device Configuration SINGLE GPU ONLY ---
# This configuration is only for single GPU training. For multi-GPU training, use the next configuration.
device: CPU # Training device. Options: [CPU, CUDA, MPS] or specific GPU ID [cuda:0, cuda:1, ...]

# --- Model Saving Configurations ---
save_final_model: True # Whether to save the model after training.
save_model_path: null # Path to save the trained model.
save_model_name: null # Name of the saved model.

# --- Checkpoint Saving Configurations ---
save_checkpoint: false # Whether to save the model checkpoint during training.
save_checkpoint_path: null # Path to save the model checkpoint.
save_checkpoint_name: null # Name of the saved model checkpoint.
checkpoint_interval: 1 # Interval epochs to save the model checkpoint.

# --- Checkpoint Loading Configurations ---
load_checkpoint: false # Whether to load the model checkpoint before training.
load_checkpoint_path: null # Path to load the model checkpoint.

# --- Other Configurations ---
verbose: True # Whether to print the training progress.
