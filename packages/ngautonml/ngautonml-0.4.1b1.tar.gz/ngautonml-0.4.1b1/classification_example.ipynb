{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57beea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    dirpath = Path(globals()['_dh'][0])\n",
    "except KeyError:\n",
    "    dirpath = Path(__file__).parent\n",
    "sys.path.append(str(dirpath))\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ae8115",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "At this time, the only installation available is via cloning the repository on GitLab:\n",
    "```\n",
    "git clone git@gitlab.com:autonlab/ngautonml.git\n",
    "```\n",
    "As the project leaves Alpha stage, a Pypi package will be posted for easy installation.\n",
    "\n",
    "It is recommended to create a virtual environment to run ngautonml.  To do so with conda, run:\n",
    "```\n",
    "conda create -n env-name python=3.9\n",
    "conda activate env-name\n",
    "```\n",
    "\n",
    "ngAutonML is designed to run on Python 3.9 and above.\n",
    "\n",
    "A ```requirements.txt``` file is provided to install necessary libraries. Use:\n",
    "\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b294e91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from ngautonml.algorithms.impl.algorithm_auto import AlgorithmCatalogAuto\n",
    "from ngautonml.executor.simple.simple_executor import SimpleExecutor\n",
    "from ngautonml.generator.generator import GeneratorImpl\n",
    "from ngautonml.instantiator.instantiator_factory import InstantiatorFactory\n",
    "from ngautonml.metrics.impl.metric_auto import MetricCatalogAuto\n",
    "from ngautonml.problem_def.problem_def import ProblemDefinition\n",
    "from ngautonml.ranker.ranker_impl import RankerImpl\n",
    "from ngautonml.templates.impl.template_auto import TemplateCatalogAuto\n",
    "from ngautonml.wrangler.wrangler import Wrangler\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0036e51f",
   "metadata": {},
   "source": [
    "<h2>Problem Definition</h2>\n",
    "\n",
    "All values are case-independent. The data format is `json`.\n",
    "\n",
    "The dataset clause currently only supports a `config` of `local`. This is for loading from a local file.\n",
    "\n",
    "Supported data formats for the `train_path` are `csv` and `arff`.\n",
    "\n",
    "In `column_roles` we identify the `class` column as the target.\n",
    "\n",
    "For `problem_type` we support `data_types` of `tabular` and `timeseries`. The supported `task` options as of 2023-07 are `binary_classification`, `regression`, and `forecasting`. Coming soon is `multiclass_classification`.\n",
    "\n",
    "Currently, for classification problems, the `accuracy_score` and `roc_auc_score` metrics are supported, and they have no parameters. Future metrics may specify parameters.\n",
    "\n",
    "We currently output predictions, trained models, and instantiations to memory. we will be saving these outputs to disk soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca64aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_definition = ProblemDefinition(\n",
    "'{'\n",
    "'    \"_comments\" : ['\n",
    "'        \"A json file fully encapsulating the problem definition for openml dataset #31.\",'\n",
    "'        \"This dataset is a tabular binary classification problem.\",'\n",
    "'        \"People are classified as good or bad credit risks based on attributes.\"'\n",
    "'    ],'\n",
    "'    \"dataset\" : {'\n",
    "'        \"config\" : \"local\",'\n",
    "f'        \"test_path\": \"{dirpath}/examples/classification/credit-test.csv\",'\n",
    "f'        \"train_path\" : \"{dirpath}/examples/classification/credit-train.csv\",'\n",
    "'        \"column_roles\": {'\n",
    "'            \"target\": {'\n",
    "'                \"name\": \"class\"'\n",
    "'            }'\n",
    "'        }'\n",
    "'    },'\n",
    "'    \"problem_type\" : {'\n",
    "'        \"data_type\": \"TABULAR\",'\n",
    "'        \"task\": \"BINARY_CLASSIFICATION\"'\n",
    "'    },'\n",
    "'    \"metrics\" :  {'\n",
    "'        \"accuracy_score\": {},'\n",
    "'        \"roc_auc_score\": {}'\n",
    "'    },'\n",
    "'    \"output\" : {'\n",
    "'        \"_comments\" : ['\n",
    "'            \"uncomment these lines if you want to save predictions and instantiations to disk.\",'\n",
    "'            {'\n",
    "f'               \"path\" : \"{dirpath}/examples/classification/credit-output\",'\n",
    "'                \"instantiations\": ['\n",
    "'                    \"SIMPLE\"'\n",
    "'                ]'\n",
    "'            }'\n",
    "'        ]'\n",
    "'    },'\n",
    "'    \"hyperparams\": ['\n",
    "'        \"disable_grid_search\"'\n",
    "'    ]'\n",
    "'}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e88cce2",
   "metadata": {},
   "source": [
    "<h2>Metrics</h2>\n",
    "\n",
    "Metrics are used to rank pipelines.\n",
    "\n",
    "The `metric_catalog` loads metrics from the top level `metrics` directory. Also available is `MetricCatalogSimple` which has an explicit list of metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc6f4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_catalog = MetricCatalogAuto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d3b297",
   "metadata": {},
   "source": [
    "<h2>Algorithms</h2>\n",
    "\n",
    "Algorithms are the steps in a data science pipeline; they can be preprocessors, or prediction models that can be trained such as classifiers or regressors.\n",
    "    \n",
    "Every algorithm has `fit`, `predict`, and `save` methods. You load a saved model by passing it to the constuctor to the algorithm.\n",
    "\n",
    "The `algorithm_catalog` supports loading algorithms from the top level `models/` directory. No other configuration is needed to add a new algorithm. There is also the option of `AlgorithmCatalogSimple()` which has a predefined set of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6097564",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_catalog = AlgorithmCatalogAuto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d947e840",
   "metadata": {},
   "source": [
    "<h2>Generator</h2>\n",
    "\n",
    "The generator is responsible for converting the pipeline templates into bound pipelines, where hyperparams are set and algorithms are fully resolved to a specific algorithm catalog entry. A single pipeline template typically resolves to a set of bound pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a823f977",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = GeneratorImpl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786fcb93",
   "metadata": {},
   "source": [
    "<h2>Templates</h2>\n",
    "\n",
    "The `template_catalog` contains the template pipelines for different problem types. As of 2023-07, there are exactly two templates.\n",
    "\n",
    "You can add new templates by putting them in the `templates/` top level directory. Alternatively, there is a TemplateCatalogSimple() which has a predefined list of templates.\n",
    "\n",
    "Templates need references to the `algorithm_catalog` and the `generator` for use in the generator phase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a46bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_catalog = TemplateCatalogAuto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95bd71e",
   "metadata": {},
   "source": [
    "<h2>Ranker</h2>\n",
    "\n",
    "The `ranker` is responsible for applying `metrics` to decide which instantiated pipeline is the best for a given metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a8efb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranker = RankerImpl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a80a655",
   "metadata": {},
   "source": [
    "<h2>Executor & Instantiator</h2>\n",
    "\n",
    "The `executor` works with the `instantiator` to turn bound pipelines into executable code. The `simple` instantiator compiles a bound pipeline into a format that can be run by the `simple` executor, a single-threaded naive execution model.\n",
    "\n",
    "Pipeline output also uses the instantiators to produce executable forms specified in the `output.instantiations` clause of the problem definition. As of 2023-07 the only supported instantiators are `simple` and `stub`, nether of which produces a persistant form, such as a file on the filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52318d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "executor = SimpleExecutor\n",
    "instantiator_factory=InstantiatorFactory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4455fa2",
   "metadata": {},
   "source": [
    "## Putting it all together: the Wrangler\n",
    "The `wrangler` puts all the pieces together. The `wrangler.fit_predict_rank()` method runs all pipelines described from the problem definition and ranks them for each requested metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39859188",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangler = Wrangler(\n",
    "    problem_definition=problem_definition,\n",
    "    metric_catalog=metric_catalog,\n",
    "    algorithm_catalog=algorithm_catalog,\n",
    "    ranker=ranker,\n",
    "    template_catalog=template_catalog,\n",
    "    generator=generator,\n",
    "    executor=executor,\n",
    "    instantiator_factory=instantiator_factory,\n",
    ")\n",
    "\n",
    "got = wrangler.fit_predict_rank()\n",
    "print(got.train_results)\n",
    "print(got.rankings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dd085c",
   "metadata": {},
   "source": [
    "## Test data\n",
    "You can specify an additional test set in the problem definition, and the wrangler will predict on it using pipelines trained on the full train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698347fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(got.test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40850c51-9a93-41d0-b0a1-44d62a4d55f4",
   "metadata": {},
   "source": [
    "## Sending more data through a pipeline\n",
    "You can also send test data through trained pipelines manually, without specifying it in the problem definition, using `wrangler.predict()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35df940e-de1d-4b73-9f7e-24c8c435da26",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = wrangler.dataset(\n",
    "    data=f'{dirpath}/examples/classification/credit-test.csv',\n",
    "    roles=['attribute'])\n",
    "\n",
    "test_predictions = wrangler.predict(new_data=test_dataset, trained_pipelines=got.executable_pipelines)\n",
    "print(test_predictions)\n",
    "\n",
    "# get rankings on new predictions by supplying ground truth\n",
    "ground_truth = wrangler.dataset(\n",
    "    data=f'{dirpath}/examples/classification/credit-test.csv',\n",
    "    key='ground_truth',\n",
    "    roles=['target'])\n",
    "\n",
    "rankings = wrangler.rank(results=test_predictions, ground_truth=ground_truth)\n",
    "\n",
    "print(rankings)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "ae537c7bad7a286962624bb706685d70455be40ce63f4435480ecca55813e257"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
