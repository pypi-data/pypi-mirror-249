{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working With Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of ways we can interact with the gradients during and after a backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, we save the hidden states of the last layer and do a backward pass on the sum of the logits.\n",
    "\n",
    "Note two things:\n",
    "\n",
    "1. We use `inference=False` in the `.forward` call to turn off inference mode. This allows gradients to be calculated. \n",
    "2. We can all `.backward()` on a value within the tracing context just like you normally would."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.5216, -1.1755, -0.4617,  ..., -1.1919,  0.0204, -2.0075],\n",
      "         [ 0.9841,  2.2175,  3.5851,  ...,  0.5212, -2.2286,  5.7334]]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "\n",
    "model = LanguageModel('gpt2', device_map='cuda')\n",
    "\n",
    "with model.forward(inference=False) as runner:\n",
    "    with runner.invoke('Hello World') as invoker:\n",
    "\n",
    "        hidden_states = model.transformer.h[-1].output[0].save()\n",
    "\n",
    "        logits = model.lm_head.output\n",
    "\n",
    "        logits.sum().backward()\n",
    "\n",
    "print(hidden_states.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to see the gradients for the hidden_states, we can call `.retain_grad()` on it and access the `.grad` attribute after execution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.5216, -1.1755, -0.4617,  ..., -1.1919,  0.0204, -2.0075],\n",
      "         [ 0.9841,  2.2175,  3.5851,  ...,  0.5212, -2.2286,  5.7334]]],\n",
      "       device='cuda:0', grad_fn=<AsStridedBackward0>)\n",
      "tensor([[[  28.7976, -282.5977,  868.7343,  ...,  120.1742,   52.2264,\n",
      "           168.6447],\n",
      "         [  79.4183, -253.6227, 1322.1290,  ...,  208.3981,  -19.5544,\n",
      "           509.9856]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "\n",
    "model = LanguageModel('gpt2', device_map='cuda')\n",
    "\n",
    "with model.forward(inference=False) as runner:\n",
    "    with runner.invoke('Hello World') as invoker:\n",
    "\n",
    "        hidden_states = model.transformer.h[-1].output[0].save()\n",
    "        hidden_states.retain_grad()\n",
    "\n",
    "        logits = model.lm_head.output\n",
    "\n",
    "        logits.sum().backward()\n",
    "\n",
    "print(hidden_states.value)\n",
    "print(hidden_states.value.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch also provides hooks into the backward process via the inputs and outputs. NNsight uses these in a similar way as  `.input` and `.output` by also providing `.backward_input` and `.backward_output`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.5216, -1.1755, -0.4617,  ..., -1.1919,  0.0204, -2.0075],\n",
      "         [ 0.9841,  2.2175,  3.5851,  ...,  0.5212, -2.2286,  5.7334]]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[[  28.7976, -282.5977,  868.7343,  ...,  120.1742,   52.2264,\n",
      "           168.6447],\n",
      "         [  79.4183, -253.6227, 1322.1290,  ...,  208.3981,  -19.5544,\n",
      "           509.9856]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "\n",
    "model = LanguageModel('gpt2', device_map='cuda')\n",
    "\n",
    "with model.forward(inference=False) as runner:\n",
    "    with runner.invoke('Hello World') as invoker:\n",
    "\n",
    "        hidden_states = model.transformer.h[-1].output[0].save()\n",
    "        hidden_states_grad = model.transformer.h[-1].backward_output[0].save()\n",
    "        logits = model.lm_head.output\n",
    "\n",
    "        logits.sum().backward()\n",
    "\n",
    "print(hidden_states.value)\n",
    "print(hidden_states_grad.value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ndif",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
