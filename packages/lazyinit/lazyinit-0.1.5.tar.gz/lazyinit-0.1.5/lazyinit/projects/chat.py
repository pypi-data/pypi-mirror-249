import lazydl as l
from lazyinit.utils import echo
import torch
import datetime


class config:
    # model_name_or_path = "THUDM/chatglm2-6b-32k"
    model_name_or_path = "lmsys/vicuna-7b-v1.5"
    model_alias = "AI å¿ƒç†å’¨è¯¢å¸ˆ"
    prompt_module = "modules.prompt_templates.psy_prompt.Prompt"
    # ç”Ÿæˆè¶…å‚é…ç½®
    max_new_tokens = 1024  # æ¯è½®å¯¹è¯æœ€å¤šç”Ÿæˆå¤šå°‘ä¸ªtoken
    history_max_len = 1000  # æ¨¡å‹è®°å¿†çš„æœ€å¤§tokené•¿åº¦
    top_p = 0.9
    temperature = 0.35
    repetition_penalty = 1.0
    # æ˜¯å¦ä½¿ç”¨4bitè¿›è¡Œæ¨ç†ï¼Œèƒ½å¤ŸèŠ‚çœå¾ˆå¤šæ˜¾å­˜ï¼Œä½†æ•ˆæœå¯èƒ½ä¼šæœ‰ä¸€å®šçš„ä¸‹é™
    load_in_4bit = False
    use_qlora = True
    
def get_time():
    return datetime.datetime.now().strftime("%mæœˆ%dæ—¥ %H:%M")




def main():
    
    model, tokenizer = l.load_model_and_tokenizer(config.model_name_or_path, use_qlora=config.use_qlora, load_in_4bit=config.load_in_4bit)
    model = model.eval()
    # model = l.load_class(config.lit_model_file)(config, tokenizer, model=model)
    prompt = l.load_class("modules.prompt_templates.psy_prompt.Prompt")

    history = []

    # å¼€å§‹å¯¹è¯
    echo("\n\n")
    l.hi()
    echo("ğŸŒˆğŸŒˆğŸŒˆ å½“å‰å¯¹è¯æ¨¡å‹ä¸ºï¼š{}ï¼Œæƒé‡æ¥è‡ªï¼š{}\n\n".format(config.model_alias, config.model_name_or_path), color="#ff9900")
    utterance_id = 0    # è®°å½•å½“å‰æ˜¯ç¬¬å‡ è½®å¯¹è¯ï¼Œä¸ºäº†å¥‘åˆchatglmçš„æ•°æ®ç»„ç»‡æ ¼å¼
    
    last_time = get_time()
    echo("                                                    " + last_time, color="#808080")
    echo("\n")
    
    user_input = input('User ï¼š')
    echo("\n")
    
    while True:
        utterance_id += 1
        input_prompt = prompt.build_prompt(
                        id=1,
                        history=history, 
                        query=user_input)
        model_input_ids = tokenizer.encode(input_prompt, return_tensors="pt", add_special_tokens=True).to(model.device)

        with torch.no_grad():
            outputs = model.generate(
                input_ids=model_input_ids, max_new_tokens=config.max_new_tokens, do_sample=True, top_p=config.top_p,
                temperature=config.temperature, repetition_penalty=config.repetition_penalty, eos_token_id=tokenizer.eos_token_id
            )
        model_input_ids_len = model_input_ids.size(1)
        response_ids = outputs[:, model_input_ids_len:]
        response = tokenizer.batch_decode(response_ids)[0]
        history = history + ["ç”¨æˆ·ï¼š" + user_input, "ç­”ï¼š" + response]
        
        echo(f"{config.model_alias} ï¼š" + response.strip().replace(tokenizer.eos_token, ""), color="#cd853f")
        echo("\n")
        if last_time != get_time():
            last_time = get_time()
            echo("                                                    " + get_time(), color="#808080")
            echo("\n")
        user_input = input('User ï¼š')
        echo("\n")


if __name__ == '__main__':
    main()

