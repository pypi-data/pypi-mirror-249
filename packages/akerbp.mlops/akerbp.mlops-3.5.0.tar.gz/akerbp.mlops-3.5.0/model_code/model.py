"""
model.py
This is a model template that shows the interface expected by MLOps framework
for an example model (scikit-learn's DummyClassifier).

Internal elements start by "_", as usually done in Python. This points to parts
of this file that are specific to the example and not required by the framework.

We will develop a single service, namely prediction/inference. Refer to the history for code needed to include a training service (was never used)
"""
import os
import sys
import yaml
import shutil
from typing import Any, Dict, List

from joblib import load
from akerbp.mlpet import Dataset, utilities
from akerbp.mlpet import feature_engineering as fe
from pathlib import Path

from akerbp.mlops.core.logger import get_logger
from akerbp.mlops.cdf.helpers import get_client
import akerbp.mlops.model_manager as mm

logger = get_logger(__name__)

_model_name = "dummy.joblib"


class ModelException(Exception):
    """
    This exception should be raised when there are errors (required by the
    framework). It can be subclassed if more specific exceptions are needed, but
    this isn't required by the framework.
    """

    pass


class ModelIO(ModelException):
    pass


class ModelDataValidation(ModelException):
    pass


def _load_model(folder_path: str) -> Any:
    """
    Load model artifact generated by `train`
    Input:
      - folder path (string) that contains model artifacts
    Output:
      - clf: model object required by `predict`. If there are multiple objects,
      you can return a tuple e.g. `(model_1, model_2)`
    """
    source_path = os.path.join(folder_path, _model_name)
    try:
        clf = load(source_path)
    except Exception as error:
        error_type = type(error).__name__
        error_message = f"Could not load model. {error_type}: {error}"
        logger.error(error_message)
        raise ModelIO(error_message) from error
    logger.info(f"Read model {clf=} from {source_path=}")
    return clf


def initialization(folder_path: str, secrets) -> Any:
    """
    Initialize objects required by `predict`.
    This can involve e.g. loading model artifacts generated by `train`,
    instantiate objects required to read data, etc.

    Input:
      - folder path (string) that contains model artifacts
      - secrets: dictionary with api keys, keys are "files" and "data"

    Output: object required by predict. If there are multiple objects,
      this could be a tuple, e.g. `(model, client)`
    """
    try:
        settings_path = os.path.abspath(
            os.path.join(folder_path, "mlpet_settings.yaml")
        )
        with open(settings_path) as f:
            settings = yaml.load(f, Loader=yaml.SafeLoader)["datasets"]

        ds = Dataset(
            settings=settings,
            folder_path=os.path.abspath("model_code"),
        )
    except FileNotFoundError:
        pass

    model = _load_model(folder_path)

    return (model, ds)


def _validate_input(data: Dict):
    """
    Validate features sent by prediction or training service API.
    Generate ModelDataValidation with a useful message for the user.
    """
    if not isinstance(data, dict):
        raise ModelDataValidation("Data should be a dictionary")
    if "input" not in data.keys():
        raise ModelDataValidation("Input dictionary should have a column 'input'")
    if not isinstance(data["input"], list):
        raise ModelDataValidation("field 'input' should contain an array")


def _process(data_dict: Dict, ds: Dataset, **process_kwargs) -> List:
    """
    Transform data provided by the prediction service API into the data
    expected by the model.
    """
    well_name = process_kwargs.get("well_name", None)
    test_case = process_kwargs.get("test_case", "default")
    secrets = process_kwargs.get("secrets", None)
    user_kwargs = process_kwargs.get("user_kwargs", None)

    # Load input data
    ds.load_from_dict(data_dict)
    ds.df_original[ds.id_column] = well_name

    # Deal with different test-cases
    if test_case != "default":
        client = get_client(
            client_id=secrets["id-read"],
            client_secret=secrets["secret-read"],
            tenant_id=secrets["tenant-id"],
            base_url=secrets["base-url"],
        )

        if test_case == "call_cdf_model":
            # Call VSH model from CDF as part of pre-processing pipeline
            ds.preprocessing_pipeline["add_petrophysical_features"] = {
                "petrophysical_features": "VSH",
                "keyword_arguments": user_kwargs,
            }
            ds.petrophysical_features = ["VSH"]

        elif test_case == "add_vertical_depths_from_CDF":
            if "CDF_wellName" not in ds.df_original.columns:
                fe.add_well_metadata(
                    ds.df_original,
                    metadata_columns=["CDF_wellName"],
                    id_column=ds.id_column,
                    client=client,
                )
            ds.df_original = fe.add_vertical_depths(
                ds.df_original,
                client=client,
                id_column="CDF_wellName",
                md_column=ds.depth_column,
            )
            assert "TVDBML" in ds.df_original.columns, "TVDBML not injected from CDF"

        elif test_case == "monkeypatch_model":
            mm.setup()
            mm.download_deployment_folder(
                model_name="automatic_vsh",
                env="prod",
                files_to_ignore=[
                    "requirements.txt",
                    "handler.py",
                    "mlops_service_settings.yaml",
                ],
                target_path=ds.folder_path,
            )

            vsh_dir = Path(__file__).parent
            if str(vsh_dir) not in sys.path:
                sys.path.append(str(vsh_dir))
            try:
                from automatic_vsh.model import initialization as vsh_init
                from automatic_vsh.model import predict as vsh_predict
            except ModuleNotFoundError as e:
                raise ModelException(
                    f"Could not import automatic_vsh. something happened during deployment folder download {e}"
                ) from e

            # Add VSH to preprocessing pipeline and deal with user kwargs
            # del ds.preprocessing_pipeline["fillna_with_fillers"]
            ds.preprocessing_pipeline["add_petrophysical_features"] = {
                "petrophysical_features": "VSH",
                "keyword_arguments": user_kwargs,
            }
            ds.petrophysical_features = ["VSH"]

            # ds.preprocessing_pipeline["fillna_with_fillers"] = {}

            # monkeypatch mlpet functionality
            def _run_monkeypatched_vsh(
                in_data,
                cdf_external_id="automatic_vsh_prediction-prod",
                client=None,
                return_file=False,
                **kwargs,
            ):
                for package in in_data["input"]:
                    if "bypass_group_formation" not in package["keyword_arguments"]:
                        if {"GROUP", "FORMATION"}.issubset(package["input_logs"]):
                            package["keyword_arguments"][
                                "bypass_group_formation"
                            ] = True
                        else:
                            package["keyword_arguments"][
                                "bypass_group_formation"
                            ] = False
                vsh_artifacts_dir = vsh_dir / "artifacts"
                vsh_model_elements = vsh_init(vsh_artifacts_dir, secrets, verbose=False)
                vsh_predictions = vsh_predict(in_data, vsh_model_elements, secrets)

                return vsh_predictions

            utilities.run_CDF_function = _run_monkeypatched_vsh

    x = ds.preprocess()

    # Remove added features
    if ds.features_added.__len__() > 0:
        x.drop(columns=ds.features_added, inplace=True)

    if test_case == "monkeypatch_model":
        # delete content of deployment folder after preprocessing
        dirs_to_delete = ["artifacts", "automatic_vsh"]
        for dir_to_delete in dirs_to_delete:
            shutil.rmtree(os.path.join("model_code", dir_to_delete))

    return x[x[ds.label_column].notna()]


def predict(data_dict, init_object, secrets) -> Any:
    """
    Predict data provided through the model service API

    Inputs:
      - data_dict: data (dictionary)
      - init_object: object provided by `load_model`. If `initialization` returns
          a tuple, you can unpack it, e.g. `model, client = init_object`
      - secrets: dictionary with api keys, keys are "files" and "data"
    Output:
      - y: prediction provided by `model` for `x`
        Note: prediction should be jsonifyable, which means for example that any
        np array should be converted to list
    """
    model, ds = init_object

    _validate_input(data_dict)

    predictions = []
    # Iterate over wells in data_dict
    test_case = data_dict.get("test_case", "default")
    for well_data in data_dict["input"]:
        kwargs = well_data.get("keyword_arguments", {})
        try:
            ds.categorical_value = kwargs["nan_textual_value"]
            ds.numerical_value = kwargs["nan_numerical_value"]
        except KeyError as e:
            raise Exception(
                "nan_textual_value and nan_numerical_value are required keyword arguments, please provide them in the payload"
            ) from e

        logs = well_data["input_logs"]

        x = _process(
            logs,
            ds,
            secrets=secrets,
            well_name=well_data["well"],
            test_case=test_case,
            user_kwargs=kwargs,
        )
        y = model.predict(x)
        response = {
            "DEPTH": logs["DEPTH"],
            "well_name": "test_well",
            "CALI": y.tolist(),
        }
        predictions.append(response)
    return predictions
