"""
test_model.py
The MLOps framework needs `ServiceTest` to test services locally and make
test calls after deploying services.
It's highly recommended to define tests for training, predictions and data
validation, but it's not required by the framework.
"""
import os
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Any, List, Dict

import pytest

import sys
from model_code.model import ModelIO, initialization, predict
from akerbp.mlops.core.config import client_secrets as secrets

sys.path.append(os.getcwd())


class ServiceTest:
    """
    MLOps framework uses this object to test services
    Training-related methods and attributes are not required by the framework
    if there's no training service.
    """

    def __init__(self):
        """
        Define known inputs: `training_input` and `prediction_input`.
        Note that the values need to be customized to your model.
        """

        input_test = pd.read_parquet(
            Path(__file__).parent.parent / "model_artifact" / "input_test.pq"
        )
        well = input_test["well_name"].unique()[0]
        input_test = input_test[input_test["well_name"] == well]
        input_test = input_test.drop(columns=["well_name"])

        self.target_column = "CALI"

        well_input = {
            "well": well,
            "input_logs": input_test.to_dict(orient="list"),
            "keyword_arguments": {
                "nan_numerical_value": -9999,
                "nan_textual_value": "-9999.0",
            },
        }

        well_input_file_reference = {
            "well": well_input["well"],
            "input_logs": {},
            "input_file_reference": "",  # Filled in at run-time
            "keyword_arguments": well_input["keyword_arguments"],
        }

        self.prediction_input: Dict[str, Any] = {
            "input": [well_input],
            "test_case": "default",
            "return_file": False,
        }

        self.prediction_input_file_reference = {
            "input": [well_input_file_reference],
            "test_case": "default",
            "return_file": False,
        }

    def prediction_check(self, y: List):
        """
        Evaluate the prediction service's output when the input is
        `self.prediction_input`
        Input:
        - y: output generated by `predict`
        Output:
        - `True` if `y` is correct, `False` otherwise.
        """
        nan_numerical = self.prediction_input["input"][0]["keyword_arguments"][
            "nan_numerical_value"
        ]
        target = [
            val
            for val in self.prediction_input["input"][0]["input_logs"][
                self.target_column
            ]
            if val != nan_numerical
        ]
        mean_target = np.mean(target)

        if np.allclose(y, [mean_target] * len(y)):
            return True
        else:
            return False


################################################################################
# The rest of the f le are tests for the example model. It's recommended that
# you include tests to check your code. They aren't required by the framework,
# but if they're present they will be run by pytest during deployment
################################################################################

# We will reuse the inputs and checks in the unit tests. Not required though.
service_test = ServiceTest()  # type: ignore


def _check_model(model: Any) -> None:
    x = service_test.prediction_input
    y = predict(x, model, secrets)[0]["prediction"]
    assert service_test.prediction_check(y)


def test_model_loading_error():
    with pytest.raises(ModelIO):
        initialization(folder_path="path_doesnt_exist", secrets=secrets)


def test_predict():
    model = initialization("model_artifact", secrets=secrets)
    _check_model(model)


def test_predict_local_model():
    """
    Test prediction service using a model artifact to load the model.
    This will fail in the training service, since model artifacts are not
    deployed in this case. It's important to skip this test as shown below:
    """
    if os.getenv("SERVICE_NAME") == "training":
        pytest.skip("This test is specific to prediction services")
    model = initialization(
        "model_artifact", secrets=secrets
    )  # Same as in settings file!
    _check_model(model)


def test_predict_with_return_file_set_to_true():
    model = initialization("model_artifact", secrets=secrets)
    x = service_test.prediction_input
    x["return_file"] = True
    y = predict(x, model, secrets)[0]["prediction"]
    assert service_test.prediction_check(y)


def test_predict_call_another_model_on_cdf():
    model = initialization("model_artifact", secrets=secrets)
    x = service_test.prediction_input
    x["test_case"] = "call_cdf_model"
    y = predict(x, model, secrets)[0]["prediction"]
    assert service_test.prediction_check(y)


def test_predict_monkeypatch_model():
    model = initialization("model_artifact", secrets=secrets)
    x = service_test.prediction_input
    x["test_case"] = "monkeypatch_model"
    y = predict(x, model, secrets)[0]["prediction"]
    assert service_test.prediction_check(y)


def test_predict_add_vertical_depths_from_CDF():
    model = initialization("model_artifact", secrets=secrets)
    x = service_test.prediction_input
    x["test_case"] = "add_vertical_depths_from_CDF"
    y = predict(x, model, secrets)[0]["prediction"]
    assert service_test.prediction_check(y)


def test_predict_with_external_log_fetching():
    model = initialization("model_artifact", secrets=secrets)
    x = service_test.prediction_input
    # TODO:  Modify x to fetch logs from external sources, check if this is possible without modifying the model code
    y = predict(x, model, secrets)[0]["prediction"]
    assert service_test.prediction_check(y)
