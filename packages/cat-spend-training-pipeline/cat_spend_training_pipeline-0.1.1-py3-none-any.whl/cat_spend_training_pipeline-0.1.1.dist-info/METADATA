Metadata-Version: 2.1
Name: cat-spend-training-pipeline
Version: 0.1.1
Summary: 
Author: danilog
Requires-Python: >=3.9,<3.10
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.9
Requires-Dist: Jinja2 (==3.0.1)
Requires-Dist: fire (>=0.5.0,<0.6.0)
Requires-Dist: hopsworks (==3.4.3)
Requires-Dist: lightgbm (>=4.2.0,<5.0.0)
Requires-Dist: matplotlib (>=3.8.2,<4.0.0)
Requires-Dist: pyarrow (>=11.0.0,<12.0.0)
Requires-Dist: python-dotenv (>=1.0.0,<2.0.0)
Requires-Dist: seaborn (>=0.13.1,<0.14.0)
Requires-Dist: sktime (>=0.25.0,<0.26.0)
Requires-Dist: wandb (>=0.16.1,<0.17.0)
Description-Content-Type: text/markdown

# Training Pipeline

Check out [Lesson 2](https://medium.com/towards-data-science/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee) on Medium to better understand how we built the training pipeline.

## Install for Development

Create virtual environment:
```shell
cd training-pipeline
poetry shell
poetry install
```

Check the [Set Up Additional Tools](https://github.com/iusztinpaul/energy-forecasting#-set-up-additional-tools-) and [Usage](https://github.com/iusztinpaul/energy-forecasting#usage) sections to see **how to set up** the **additional tools** and **credentials** you need to run this project.


## Usage for Development

</br> **Run the scripts in the following order:** </br></br>


1. Start the hyperparameter tuning script:
```shell
python -m training_pipeline.hyperparameter_tuning
```

2. Upload the best config based on the previous hyperparameter tuning step:
```shell
python -m training_pipeline.best_config
```
3. Start the training script using the best configuration uploaded one step before:
```shell
python -m training_pipeline.train
```

**NOTE:** Be careful to complete the `.env` file and set the `ML_PIPELINE_ROOT_DIR` variable as explained in the [Set Up the ML_PIPELINE_ROOT_DIR Variable](https://github.com/iusztinpaul/energy-forecasting#set-up-the-ml_pipeline_root_dir-variable) section of the main README.

